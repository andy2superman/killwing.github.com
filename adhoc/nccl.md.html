<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>NCCL</title>
<xmp theme="united" style="display:none;">
https://github.com/NVIDIA/nccl

collective operations:

* AllReduce
* Broadcast
* Reduce
* AllGather
* ReduceScatter

## API
### Communicator
如果是单进程多设备之间通信，可以这样创建通信点：
```
ncclResult_t ncclCommInitAll(ncclComm_t* comm, int ndev, const int* devlist) {
  ncclUniqueId Id;
  ncclGetUniqueId(&Id); // 此通信组ID
  ncclGroupStart();
  for (int i=0; i<ndev; i++) {
    cudaSetDevice(devlist[i]);
    ncclCommInitRank(comm+i, ndev, Id, i);
  }
  ncclGroupEnd();
}
```

### Stream
collective 操作通过每个设备上的 stream 进行异步调用，用 `cudaStreamSynchronize` 等待操作完成。

### Group
如果在单线程中操作多个设备，必须使用组语义，否则会互相等待发生死锁。
```
ncclGroupStart();
for (int i=0; i<nLocalDevs; i++) {
  // 2.x 中不需要 cudaSetDevice
  ncclAllReduce(..., comm[i], stream[i]);
}
ncclGroupEnd();
```

## Test
### 多机多卡测试 (Multiple Devices per Thread)
```
#include <stdio.h>
#include <stdlib.h>
#include <cuda_runtime.h>
#include <nccl.h>
#include <mpi.h>
#include <unistd.h>
#include <stdint.h>

#define MPICHECK(cmd) do {                          \
  int e = cmd;                                      \
  if( e != MPI_SUCCESS ) {                          \
    printf("Failed: MPI error %s:%d '%d'\n",        \
        __FILE__,__LINE__, e);   \
    exit(EXIT_FAILURE);                             \
  }                                                 \
} while(0)

#define CUDACHECK(cmd) do {                         \
  cudaError_t e = cmd;                              \
  if( e != cudaSuccess ) {                          \
    printf("Failed: Cuda error %s:%d '%s'\n",             \
        __FILE__,__LINE__,cudaGetErrorString(e));   \
    exit(EXIT_FAILURE);                             \
  }                                                 \
} while(0)

#define NCCLCHECK(cmd) do {                         \
  ncclResult_t r = cmd;                             \
  if (r!= ncclSuccess) {                            \
    printf("Failed, NCCL error %s:%d '%s'\n",             \
        __FILE__,__LINE__,ncclGetErrorString(r));   \
    exit(EXIT_FAILURE);                             \
  }                                                 \
} while(0)

static uint64_t getHostHash(const char* string) {
  // Based on DJB2, result = result * 33 + char
  uint64_t result = 5381;
  for (int c = 0; string[c] != '\0'; c++){
    result = ((result << 5) + result) + string[c];
  }
  return result;
}

static void getHostName(char* hostname, int maxlen) {
  gethostname(hostname, maxlen);
  for (int i=0; i< maxlen; i++) {
    if (hostname[i] == '.') {
        hostname[i] = '\0';
        return;
    }
  }
}

int main(int argc, char* argv[])
{
  int size = 32*1024*1024;

  int myRank, nRanks, localRank = 0;

  //initializing MPI
  MPICHECK(MPI_Init(&argc, &argv));
  MPICHECK(MPI_Comm_rank(MPI_COMM_WORLD, &myRank));
  MPICHECK(MPI_Comm_size(MPI_COMM_WORLD, &nRanks));

  //calculating localRank which is used in selecting a GPU
  uint64_t hostHashs[nRanks];
  char hostname[1024];
  getHostName(hostname, 1024);
  hostHashs[myRank] = getHostHash(hostname);
  MPICHECK(MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, hostHashs, sizeof(uint64_t), MPI_BYTE, MPI_COMM_WORLD));
  for (int p=0; p<nRanks; p++) {
     if (p == myRank) break;
     if (hostHashs[p] == hostHashs[myRank]) localRank++;
  }
  printf("localRank %d, myRank %d\n", localRank, myRank);

  //each process is using two GPUs
  int nDev = 2;

  float** sendbuff = (float**)malloc(nDev * sizeof(float*));
  float** recvbuff = (float**)malloc(nDev * sizeof(float*));
  cudaStream_t* s = (cudaStream_t*)malloc(sizeof(cudaStream_t)*nDev);

  //picking GPUs based on localRank
  for (int i = 0; i < nDev; ++i) {
    CUDACHECK(cudaSetDevice(localRank*nDev + i));
    CUDACHECK(cudaMalloc(sendbuff + i, size * sizeof(float)));
    CUDACHECK(cudaMalloc(recvbuff + i, size * sizeof(float)));
    CUDACHECK(cudaMemset(sendbuff[i], 1, size * sizeof(float)));
    CUDACHECK(cudaMemset(recvbuff[i], 0, size * sizeof(float)));
    CUDACHECK(cudaStreamCreate(s+i));
  }

  ncclUniqueId id;
  ncclComm_t comms[nDev];

  //generating NCCL unique ID at one process and broadcasting it to all
  if (myRank == 0) ncclGetUniqueId(&id);
  MPICHECK(MPI_Bcast((void *)&id, sizeof(id), MPI_BYTE, 0, MPI_COMM_WORLD));

  //initializing NCCL, group API is required around ncclCommInitRank as it is
  //called across multiple GPUs in each thread/process
  NCCLCHECK(ncclGroupStart());
  for (int i=0; i<nDev; i++) {
     CUDACHECK(cudaSetDevice(localRank*nDev + i));
     NCCLCHECK(ncclCommInitRank(comms+i, nRanks*nDev, id, myRank*nDev + i));
  }
  NCCLCHECK(ncclGroupEnd());

  //calling NCCL communication API. Group API is required when using
  //multiple devices per thread/process
  NCCLCHECK(ncclGroupStart());
  for (int i=0; i<nDev; i++)
     NCCLCHECK(ncclAllReduce((const void*)sendbuff[i], (void*)recvbuff[i], size, ncclFloat, ncclSum, comms[i], s[i]));
  NCCLCHECK(ncclGroupEnd());

  //synchronizing on CUDA stream to complete NCCL communication
  for (int i=0; i<nDev; i++)
      CUDACHECK(cudaStreamSynchronize(s[i]));

  //freeing device memory
  for (int i=0; i<nDev; i++) {
     CUDACHECK(cudaFree(sendbuff[i]));
     CUDACHECK(cudaFree(recvbuff[i]));
  }

  //finalizing NCCL
  for (int i=0; i<nDev; i++) {
     ncclCommDestroy(comms[i]);
  }

  //finalizing MPI
  MPICHECK(MPI_Finalize());

  printf("[MPI Rank %d] Success \n", myRank);
  return 0;
}
```

编译：`gcc mpi-test.c -I/usr/lib/openmpi/include/ -lmpi -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcuda -lcudart -lnccl -o mpi-test`
执行：`mpirun -np 2 --allow-run-as-root -H 172.16.41.2,172.16.41.3 ./mpi-test`

### [Perf](https://github.com/nvidia/nccl-tests)
`mpirun -x NCCL_TREE_THRESHOLD=0 -x NCCL_DEBUG=INFO -np 2 --allow-run-as-root -H 172.16.41.2,172.16.41.3 ./all_reduce_perf -b 8 -e 128M -f 2 -g 2`

### Test Image
```
FROM nvidia/cuda:9.2-cudnn7-devel
RUN sed -i s/archive.ubuntu.com/mirrors.163.com/g /etc/apt/sources.list
RUN sed -i s/security.ubuntu.com/mirrors.163.com/g /etc/apt/sources.list
RUN rm /etc/apt/sources.list.d/cuda.list /etc/apt/sources.list.d/nvidia-ml.list

# NCCL
COPY nccl-repo-ubuntu1604-2.2.13-ga-cuda9.2_1-1_amd64.deb /
RUN dpkg -i nccl-repo-ubuntu1604-2.2.13-ga-cuda9.2_1-1_amd64.deb
RUN apt-key add /var/nccl-repo-2.2.13-ga-cuda9.2/7fa2af80.pub
RUN apt update
RUN apt install -y --no-install-recommends ca-certificates wget vim curl git unzip build-essential cmake \
    iputils-ping netcat dstat tcpdump htop dnsutils openssh-server net-tools sysstat iproute2 ethtool pciutils lsof \
    libnccl2 libnccl-dev

# ssh
RUN echo | ssh-keygen -q -N "" && cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys && chmod 400 /root/.ssh/id_rsa && mkdir -p /var/run/sshd

# IB
#  Set MOFED directory, image and working directory
ENV MOFED_DIR MLNX_OFED_LINUX-4.2-1.0.0.0-ubuntu16.04-x86_64
ENV MOFED_SITE_PLACE MLNX_OFED-4.2-1.0.0.0
ENV MOFED_IMAGE MLNX_OFED_LINUX-4.2-1.0.0.0-ubuntu16.04-x86_64.tgz
WORKDIR /tmp/
# Pick up some MOFED dependencies
RUN apt-get install -y --no-install-recommends \
        perl \
        lsb-release \
        libnl-route-3-200 \
        kmod \
        libnuma1 \
        linux-headers-4.4.0-92-generic \
        python-libxml2
# Download and install Mellanox OFED 4.2.1 for Ubuntu 16.04
RUN wget http://content.mellanox.com/ofed/${MOFED_SITE_PLACE}/${MOFED_IMAGE} && \
        tar -xzvf ${MOFED_IMAGE} && \
        ${MOFED_DIR}/mlnxofedinstall --user-space-only --without-fw-update --all -q && \
        cd .. && \
        rm -rf ${MOFED_DIR} && \
        rm -rf *.tgz

# mpi
RUN apt install -y libopenmpi-dev openmpi-bin openmpi-doc fio
# nccl-tests
WORKDIR /root/
RUN git clone https://github.com/NVIDIA/nccl-tests.git
RUN cd nccl-tests && make MPI=1 MPI_HOME=/usr/lib/openmpi/

# local test
COPY ./tests /root/
RUN cd /root && make

CMD ["/usr/sbin/sshd", "-D"]
```

## Reference
https://developer.nvidia.com/nccl
http://on-demand.gputechconf.com/gtc/2017/presentation/s7155-jeaugey-nccl.pdf

</xmp>
<script src="../js/strapdown.js"></script>
</html>
