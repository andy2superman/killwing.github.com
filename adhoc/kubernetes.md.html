<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>Kubernetes</title>
<xmp theme="united" style="display:none;" toc="true">
https://kubernetes.io

## 安装部署
### kubeadm
#### 安装
所有 node 上执行。
```
apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
```

最新稳定版：`https://dl.k8s.io/release/stable-1.txt` (v1.13.2)
镜像：`kubeadm config images list`
```
k8s.gcr.io/kube-apiserver:v1.13.2
k8s.gcr.io/kube-controller-manager:v1.13.2
k8s.gcr.io/kube-scheduler:v1.13.2
k8s.gcr.io/kube-proxy:v1.13.2
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
k8s.gcr.io/coredns:1.2.6
```

手动拉镜像并重命名
```
#!/bin/bash
images=`kubeadm config images list`
for i in $images; do
	image=registry.cn-hangzhou.aliyuncs.com/google_containers/${i#k8s.gcr.io/}
	docker pull $image
	docker tag $image $i
done
```

#### init master
`kubeadm init --config config.yaml`
默认配置：`kubeadm config print init/join-defaults`
详细说明：https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta1

config.yaml
```
apiVersion: kubeadm.k8s.io/v1beta1
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.212.107
  bindPort: 6443
nodeRegistration:
  kubeletExtraArgs:
    dynamic-config-dir: /var/lib/kubelet/dynamic-config
---
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
apiServer:
  extraArgs:
    enable-admission-plugins: NodeRestriction,ExtendedResourceToleration
networking:
  podSubnet: "10.100.0.0/16" # 需与 calico 配置一致，且不与 host 网络冲突
imageRepository: "registry.cn-hangzhou.aliyuncs.com/google_containers"
kubernetesVersion: "v1.13.2"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
featureGates:
    DynamicKubeletConfig: true
```

网络插件
```
kubectl apply -f https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
```

#### join node
`kubeadm token create --print-join-command`
`kubeadm join <ip>:<port> --token <token> --discovery-token-ca-cert-hash <hash>`

#### 删除 node
```
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
kubeadm reset
```

reset iptable:
`iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X`

#### 更新配置
组件：直接更新 `/etc/kubernetes/manifests` 配置
kubelet: 使用[configMap](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), 必须一开始加上 `--dynamic-config-dir` 和开启 `DynamicKubeletConfig` feature gate （默认true）。

#### 相关配置
`/etc/kubernetes/*.conf` 各组件访问 APIServer 鉴权文件。
`/etc/kubernetes/pki/` ca公钥私钥。
`/etc/kubernetes/manifests` 各组件静态 Pod。
`/var/lib/kubelet/kubeadm-flags.env` kubeadm 为 kubelet 设置的 `KUBELET_EXTRA_ARGS`。
`/var/lib/kubelet/config.yaml` kubelet 本地 configMap 配置文件。
`/etc/default/kubelet` kubelet `KUBELET_EXTRA_ARGS=` 配置。
`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` kubelet systemd 配置，启动命令行。

### 其它插件
[nvidia-device-plugin](https://github.com/NVIDIA/k8s-device-plugin)
`kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml`

[dashboard](https://github.com/kubernetes/dashboard)
`kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml`

[rook](https://github.com/rook/rook)
```
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
```

[kube-prometheus](https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus)

[logging](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch)

### Tips
kubelet alias 自动补全(Linux)：
`source <(kubectl completion bash | sed 's/kubectl/kc/g')`

plugins:
kubectl-ctx [kubectx](https://github.com/ahmetb/kubectx)
kubectl-ns [kubens](https://github.com/ahmetb/kubectx)
kubectl-tail [stern](https://github.com/wercker/stern)
kubectl-debug [kubectl-debug](https://github.com/aylei/kubectl-debug)

### 验证
#### [Node Conformance Test](https://kubernetes.io/docs/setup/node-conformance/)
在 node 加入集群之前验证。
run: `test/e2e_node/conformance/run_test.sh`

#### Cluster
validate cluster: `KUBECTL_PATH=$(which kubectl) NUM_NODES=1 KUBERNETES_PROVIDER=local cluster/validate-cluster.sh`

#### 其他
https://github.com/heptio/sonobuoy
https://github.com/bloomberg/powerfulseal

## Networking
Networking Model:

* all containers can communicate with all other containers without NAT
* all nodes can communicate with all containers (and vice-versa) without NAT
* the IP that a container sees itself as is the same IP that others see it as

### 网络方案
#### Overlay (UDP/VXLAN)
[![img](https://cdn-images-1.medium.com/max/1600/1*JqSLd3cPv14BWDtE7YEcRA.png)](https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c)

[flannel](https://github.com/coreos/flannel)

#### Routing (Host-GW)
![img](https://feisky.gitbooks.io/sdn/container/calico/calico.png)

[calico](https://www.projectcalico.org/)

组件：

[bird](http://bird.network.cz/): 使用 [BGP](https://www.projectcalico.org/why-bgp/) 协议，在节点之间共享路由信息。
[felix](https://github.com/projectcalico/felix): 监听 etcd 中的网络配置，根据配置更新节点的路由和 iptables。
[confd](https://github.com/kelseyhightower/confd): 监听 etcd 的数据，实时更新 bird 的配置文件，并重新启动 bird 进程以加载最新配置。
[controller](https://github.com/projectcalico/kube-controllers): 使用独立的 etcd 需要，使其中的数据和 k8s 同步。
[cni](https://github.com/projectcalico/cni-plugin): k8s 网络管理插件（同时管理 `WorkloadEndpoint` 资源）（纯 docker 使用 [libnetwork](https://github.com/projectcalico/libnetwork-plugin)）。

 * calico 主插件
 * calico-ipam ip分配
 * `/etc/cni/net.d/10-calico.conflist`
```
{
    "name": "k8s-pod-network",
    "cniVersion": "0.3.0",
    "plugins": [
      {
        "type": "calico", # 插件名字
        "log_level": "info",
        "datastore_type": "kubernetes",
        "nodename": "node1",
        "mtu": 1440,
        "ipam": {
          "type": "host-local", # 使用本地 ipam 插件 而不是 calico-ipam
          "subnet": "usePodCidr"
        },
        "policy": {
            "type": "k8s"
        },
        "kubernetes": {
            "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
        }
      },
      {
        "type": "portmap",
        "snat": true,
        "capabilities": {"portMappings": true}
      }
    ]
}
```

资源：

* HostEndpoint: 对应 node 上的网络接口。
* WorloadEndpoint: 对应 pod 的 veth 网络接口（`cali*`），区分 namespace。
* Profile: 应用于某个单独的 endpoint，每个 endpoint 可以有多个 profile。
* NetworkPolicy: 通过 label selector 应用于一组 endpoint (label 从 pod 复制)，对应于 k8s 的 `NetworkPolicy`。

### [CNI](https://github.com/containernetworking/cni)
[spec](https://github.com/containernetworking/cni/blob/master/SPEC.md) / [k8s cni](https://github.com/kubernetes/community/blob/master/contributors/devel/kubelet-cri-networking.md)

功能：

* 在容器网络命名空间中配置网络接口。
* 在host上配置网络。
* 给容器中的网络接口分配IP地址以及路由。

[标准基础插件](https://github.com/containernetworking/plugins)：

* main: 创建网络设备
* IPAM: IP地址分配
* meta: 其他(内置flannel)

plugin位置：`/opt/cni/bin`
配置(Network Configuration Lists)：`/etc/cni/net.d` 只会加载按字母顺序第一个，如 `10-flannel.conflist`

执行环境：由 CRI (如dockershim) 调用: [`RunPodSandbox` -> `SetUpPod`](https://github.com/kubernetes/kubernetes/blob/4dc40aabfb9af68913986a2caf87906312c9f4ea/pkg/kubelet/dockershim/docker_sandbox.go#L177)

执行参数：

* 环境变量：`CNI_*`
  * `CNI_COMMAND`: `ADD`/`DEL`/`CHECK`/`VERSION`
  * `CNI_CONTAINERID`: 容器ID
  * `CNI_NETNS`: 网络命名空间路径, `/proc/<container_pid>/ns/net`
  * `CNI_IFNAME`: 容器内网络接口名字
  * `CNI_ARGS`: KV形式的自定义参数
  * `CNI_PATH`: plugin搜索路径
* stdin: `Network Configuration` 调用中可能会被修改
```
{
    "cniVersion": "0.4.0",
    "name": "dbnet",
    "type": "bridge",
    // type (plugin) specific
    "bridge": "cni0",
    "ipam": {
      "type": "host-local",
      // ipam specific
      "subnet": "10.1.0.0/16",
      "gateway": "10.1.0.1"
    },
    "dns": {
      "nameservers": [ "10.1.0.1" ]
    }
}
```
`delegate` 字段（内容和 `Network Configuration` 相同）可以用于委派其他插件完成。

执行结果(`ADD`)：

* result code: 1/2/3/11
* stdout: `Result` (`IPAM`的结果没有interfaces)
```
{
    "cniVersion": "0.4.0",
    "interfaces": [...],
    "ips": [...],
    "routes": [...],
    "dns": [...]
}
```
失败结果：
```
{
    "cniVersion": "0.4.0",
    "code": <numeric-error-code>,
    "msg": <short-error-message>,
    "details": <long-error-message> (optional)
}
```

整体List执行：
`ADD`按照`Network Configuration Lists`配置的plugins顺序执行，`DEL`反序执行。
```
{
    "cniVersion": "0.4.0",
    "name": "...",
    "disableCheck": false,
    "plugins": [<Network Configuration>, ...]
}
```

对于 `ADD`, 把结果 `Result` 作为`Network Configuration` 的一个新增字段 `prevResult` 传给下一个plugin。后面的 plugin 除非需要修改或覆盖结果，都应该把收到的 `prevResult` 原样输出给下一个。
如果 `ADD` 中有一个 plugin 失败，执行 `DEL` (全部 plugins) 作为错误处理。
`DEL` 输入也包含最近 `ADD` 的 `Result` 作为 `prevResult`。

## Storage
### [CSI](https://kubernetes-csi.github.io/)
[spec](https://github.com/container-storage-interface/spec/blob/master/spec.md) / [k8s csi](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md)

[![img](https://cdn-images-1.medium.com/max/1000/1*oMgMPjx0obXKlaItZOkRfA.png)](https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b)

external components (从 kubelet 剥离，由 k8s 维护):

* [node driver registrar](https://github.com/kubernetes-csi/node-driver-registrar): [registering] 向 kubelet 注册 CSI 驱动（利用 deivce plugin/KubeletPluginsWatcher 的注册机制），触发 CSI Identity 端点的 `GetPluginInfo/GetPluginCapabilities` 方法。安装驱动可执行文件。以及健康检测。
* [cluster driver registrar](https://github.com/kubernetes-csi/cluster-driver-registrar): 为驱动创建 `CSIDriver` 对象。自定义一些行为（attachRequired/podInfoOnMountVersion）。（非必要）
* [external provisioner](https://github.com/kubernetes-csi/external-provisioner): [provisioning] 监听 `PersistentVolumeClaim` 对象，触发 CSI 端点的 `CreateVolume/DeleteVolume` 方法创建实际的卷，进而创建出对应的 `PersistentVolume`。`StorageClass.provisioner` 要和 CSI 端点 `GetPluginInfo` 返回的一致。
* [external attacher](https://github.com/kubernetes-csi/external-attacher): [attaching] 监听 `VolumeAttachment` 对象（当 pod 调度到 node 时，由 kube-contoller-manager 创建），触发 CSI Controller 端点的 `ControllerPublishVolume/Unpublish` 方法，更新 `VolumeAttachment` 状态为 `Attached`。

挂载的一般流程：(registering) -> provisioning -> attaching -> mounting
[mounting] kubelet 监听 `VolumeAttachment` 的状态，当为 `Attached` 时，触发 CSI Node 端点的 `NodeStageVolume/NodeUnstageVolume`格式化准备和 `NodePublishVolume/NodeUnpublishVolume` 方法进行实际的挂载。

csi driver / plugin （第三方提供）：

* CSI Identity: 提供驱动的标识和基本信息。

```
service Identity {
  rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {}
  rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {}
  rpc Probe (ProbeRequest) returns (ProbeResponse) {}
}
```

* CSI Controller: 提供卷的基本服务：创建，删除，attach/dettach (publish/unpublish，指将设备挂到node上，一般只有块存储需要), snapshoting 等。（volume contoller逻辑，不需要在node上操作，部署在master节点上）。

```
service Controller {
  rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {}
  rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {}
  rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {}
  rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {}
  rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest) returns (ValidateVolumeCapabilitiesResponse) {}
  rpc ListVolumes (ListVolumesRequest) returns (ListVolumesResponse) {}
  rpc GetCapacity (GetCapacityRequest) returns (GetCapacityResponse) {}
  rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest) returns (ControllerGetCapabilitiesResponse) {}
  rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {}
  rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {}
  rpc ListSnapshots (ListSnapshotsRequest) returns (ListSnapshotsResponse) {}
  rpc ControllerExpandVolume (ControllerExpandVolumeRequest) returns (ControllerExpandVolumeResponse) {}
}
```

* CSI Node: 在node上执行的具体操作（由 kubelet 直接调用），如格式化以及 mount/unmount 存储路径到 pod volume 的 mount point 上。

```
service Node {
  rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {}
  rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {}
  rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {}
  rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {}
  rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {}
  rpc NodeExpandVolume(NodeExpandVolumeRequest) returns (NodeExpandVolumeResponse) {}
  rpc NodeGetCapabilities (NodeGetCapabilitiesRequest) returns (NodeGetCapabilitiesResponse) {}
  rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {}
}
```

部署：
![img](https://raw.githubusercontent.com/kubernetes/community/master/contributors/design-proposals/storage/container-storage-interface_diagram1.png)

(StatefulSet/Deployment) external-attacher/provisioner 通过 emptyDir 里的由 CSI Driver 创建的 socket 和其通信。
(DaemonSet) node-driver-registrar 通过 hostpath `/var/lib/kubelet/plugins_registry/[SanitizedCSIDriverName]-reg.sock` 和 kubelet 通信。
kubelet/resgistrar 通过 hostpath `/var/lib/kubelet/plugins/[SanitizedCSIDriverName]/csi.sock` 和 CSI Driver 通信。
`/var/lib/kubelet/pods/` 必须以 `mountPropagation: "Bidirectional"` 挂载进 CSI Driver，这样 CSI Driver 里的挂载在 host 上及用户容器里才能看到。


## Runtime
### [CRI](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md)

```
service RuntimeService {
    rpc Version(VersionRequest) returns (VersionResponse) {}
    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}
    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}
    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}
    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}
    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}
    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}
    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}
    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}
    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}
    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}
    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}
    rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}
    rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {}
    rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {}
    rpc Exec(ExecRequest) returns (ExecResponse) {}
    rpc Attach(AttachRequest) returns (AttachResponse) {}
    rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {}
    rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) {}
    rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) {}
    rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {}
    rpc Status(StatusRequest) returns (StatusResponse) {}
}

ervice ImageService {
    rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}
    rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}
    rpc PullImage(PullImageRequest) returns (PullImageResponse) {}
    rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}
    rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}
}
```

```
[kubelet] -- (CRI) -- [dockershim] -- [docker daemon] -- [containerd] -- (OCI) -- [runc]
                   -- [cri-containerd] ------------------^               ^
                   -- [cri-o] -------------------------------------------'
```

默认 kubelet (`--container-runtime=docker`) 通过 `--container-runtime-endpoint=/var/run/dockershim.sock`（image 默认和 runtime 相同） 和实现了 CRI 的 dockershim GRPC server 通信。
若使用 cri-o, 则配置 `--container-runtime=remote` 及 `--container-runtime-endpoint=/var/run/crio/crio.sock`。

在 docker runtime 下，Exec/Attach/PortForward 会被重定向到 kubelet 的 `/cri/` 路径下，由 streaming server 处理（可以是独立 HTTP server，实际直接注册 handler 到 kubelet 中），最终由实现了 `Runtime` 接口的 `streamingRuntime` 执行。
```
type Runtime interface {
	Exec(containerID string, cmd []string, in io.Reader, out, err io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize) error
	Attach(containerID string, in io.Reader, out, err io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize) error
	PortForward(podSandboxID string, port int32, stream io.ReadWriteCloser) error
}
```
即 `apiserver -(exec cmd)-> kubelet -(cri.Exec)-> dockershim -(redirect /cri/exec/)-> kubelet -(runtime.Exec)-> streamingServer -(exec container)-> container`


`kubeGenericRuntimeManager.SyncPod`

1. 创建并启动 sandbox 容器，使用 pause 镜像，调用 CNI 插件配置网络。
1. 按顺序创建并启动 init 容器。
1. 创建并启动普通容器。默认情况下容器共享 sandbox 的 Network/Ipc namespace。

### [cri-o](https://cri-o.io/)
* kubelet 运行 pod，pod 里的容器共享同样的 IPC/NET/PID namespace 和 cgroup。
* kubelet 通过 CRI 接口向 CRI-O daemon 发送运行 pod 的请求。
* CRI-O 使用 `containers/image` 库从 registry 拉取镜像。
* CRI-O 使用 `containers/storage` 库将镜像解压成 rootfs，存放于 COW 文件系统里。
* 当容器的 rootfs 创建后，CRI-O 使用 OCI 生成工具生成一份描述如何运行容器的 OCI 运行时 json 配置。
* CRI-O 启动 OCI 兼容的运行时（默认 [runc](https://github.com/opencontainers/runc)）根据配置来运行容器进程。
* 每个容器都被一个独立的 conmon 进程监控，监控进程还持有容器进程(PID1)的 pty 终端，处理日志和记录容器进程的退出码。
* Pod 的网络由 CNI 设置，CRI-O 可以使用任何 CNI 插件。

## Components
### APIServer

### [Scheduler](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler.md)
1. 通过 Predicates 过滤出符合条件的 node。
1. 通过 Priorities 为符合条件的 node 打分。`finalScoreNodeA = (weight1 * priorityFunc1) + (weight2 * priorityFunc2)`
1. 选择最高分的那个 node 作为目标，设置 Pod 的 `spec.nodeName`。

#### [Predicates & Priorities](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md)
Predicates
GeneralPredicates (也会被 kubelet 启动 Pod 前再次检查)：

`PodFitsResources`: 资源是否满足 [node]capacity - sum([Pod]spec.resources.requests)
`PodFitsHost`: node 名字是否和 `spec.nodeName` 一致
`PodFitsHostPorts`: host port 在 node 上是否已被占用
`PodMatchNodeSelector`: nodeSelector 或 nodeAffinity 是否和 node 匹配

Volume 相关：
`NoDiskConflict`: Pod 声明挂载的持久化 Volume 与 node 上已挂载的是否有冲突
`MaxPDVolumeCountPredicate`: node 持久化 Volume 的最大数目
`VolumeZonePredicate`: 持久化 Volume 的 Zone 标签，是否与 node 的 Zone 匹配
`VolumeBindingPredicate`: Pod 的 PVC 对应的 PV nodeAffinity, 是否与 node 匹配（没绑定 PV 的则检查是否有符合的 PV）

`PodToleratesNodeTaints`: Pod 是否能 tolerate node 的 taint
`NodeMemoryPressurePredicate` 检查 node 内存是否已经不够
`PodAffinityPredicate`: Pod 与 node 上已有 Pod 间的 affinity 和 anti-affinity

Priorities
`LeastRequestedPriority`: 选择空闲 CPU 和 Memory 最多的 node `score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2`
`BalancedResourceAllocation`:  选择调度完成后 node 各种资源分配最均衡的 `score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10`
`NodeAffinityPriority`, `TaintTolerationPriority`, `InterPodAffinityPriority`: 选择匹配字段最多的
`ImageLocalityPriority`: 选择已经有镜像缓存的 node

#### Priority & Preemption
使用：创建 `PriorityClass` 对象，在 Pod 里设置 `spec.priorityClassName` 指定。值越大越高，未设置默认`globalDefault`，Pod 未声明使用的话则 priority 为0。
调度时，优先级高的 Pod 先出队列(`activeQ`)优先调度。

抢占：当一个 Pod 调度失败时（放入`unschedulableQ`），调度器找一个 node，当 node 上一个或多个比其优先级底的 Pod 删除之后，它可以调度上去。先设置 `spec.nominatedNodeName` 为目标 node（放回`activeQ`，进入下一调度周期），然后删除 node 上的低优先级的 Pod，此时如果有更高优先级的 Pod 也来抢占这个 node，原 Pod 的`spec.nominatedNodeName`就被清除。考虑到 pod affinity, 调度新 Pod 时也需要考虑 node 上也有（或者没有）抢占 Pod 的情况（nominatedNodeName 指定了此 node）。

#### [Scheduling Framework](https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20180409-scheduling-framework.md)


### [Controller](https://git.k8s.io/community/contributors/devel/sig-api-machinery/controllers.md)
#### [client-go](https://github.com/kubernetes/client-go/)
[![img](https://raw.githubusercontent.com/kubernetes/sample-controller/master/docs/images/client-go-controller-interaction.jpeg)](https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md)

client-go 库内部组件：

* Reflector: 从 Kubernetes API 监听 (watch) 特定的资源类型 (kind)，可以是内置资源，也可以是自定义资源。当它收到新增资源对象的通知时，会把新的资源对象从 APIServer 取出 (list) 并放入 Delta Fifo 队列。
* Informer: 从 Delta Fifo 队列中取出对象放入本地 Indexer 缓存，然后调用自定义控制器注册的事件处理函数。定期同步功能，将 Indexer 中已知的对象重新放入 Delta Fifo 中。
* Indexer: 提供对象的索引功能，可以根据对象的标签创建索引提高对象查询效率。它使用一个线程安全的数据存储作为后端缓存存储对象和其键值（一般为`<namespace>/<name>`）。

自定义控制器组件：

* Resource Event Handlers: 注册在 Informer 里的针对特定资源的事件处理回调函数，通常作法是将对象的键值放入 Work Queue 以供后续处理。之所以只存键值，是因为 Indexer 里的缓存的对象可能被更新，这样处理时可以取得最新的对象。
* Work Queue: 将对象的投递和处理分离开，异步处理，起到了解藕的作用。
* Object Sync Handler: 从 Work Queue 里取出对象的键值，以从 Indexer 获得实际的对象进行处理。

### [Device Plugin](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md)

![img](https://raw.githubusercontent.com/kubernetes/community/master/contributors/design-proposals/resource-management/device-plugin-overview.png)
DP 通过 Daemonset 部署在每个 node 上。

DP 通过 `/var/lib/kubelet/device-plugins/kubelet.sock` 向 kubelet 注册:

 * DP 的版本
 * 端点名字，在 `/var/lib/kubelet/device-plugins/` 下
 * 资源名字 (Extended Resource)

```
service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}
```

kubelet 通过 `/var/lib/kubelet/device-plugins/{注册的端点名字}` 向 DP 请求：
```
service DevicePlugin {
	// returns a stream of []Device
	rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}
	rpc Allocate(AllocateRequest) returns (AllocateResponse) {}
}
```

`ListAndWatch`: 返回一组设备，包括其 ID 和健康状况。
`Allocate`: 请求一组设备（请求哪些空闲的设备是 kubelet 决定的），返回其在容器内的运行时设置。运行时设置包括: 一组环境变量，需要从 node 上 mount 的一组目录（通常包括驱动和工具文件），需要 mount 的具体设备文件。


DP 可以通过 `/var/lib/kubelet/pod-resources/kubelet.sock` 向 kubelet 查询设备资源在 node 上的分配情况。
```
// PodResources is a service provided by the kubelet that provides information about the
// node resources consumed by pods and containers on the node
service PodResources {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
}
```

KubeletPluginsWatcher: 被动注册机制，CSI/CNI 等插件使用。
kubelet 通过监控 `/var/lib/kubelet/plugins_registry/` 里面的端点向 plugin 请求进行注册：
```
service Registration {
	rpc GetInfo(InfoRequest) returns (PluginInfo) {}
	rpc NotifyRegistrationStatus(RegistrationStatus) returns (RegistrationStatusResponse) {}
}
```
PluginInfo 返回了插件类型（CSIPlugin, DevicePlugin）和插件实际 endpoint（一般在 `/var/lib/kubelet/plugins/`），后续对 plugin 的请求（如 CSI.Identity）会向这个 endpoint 发送。

## Resources
`kubectl api-resources`

### Pod
#### namesapce
默认情况下容器共享 sandbox 的 Network/Ipc namespace，共享 host 的 Cgroup/User namespace，创建独立的 PID/Mount/UTS namespace。

`spec.hostNetwork` 是否和 host 共享 Network/UTS namespace。
`spec.hostIPC` 是否和 host 共享 IPC namespace。
`spec.hostPID` 是否和 host 共享 PID namespace。
`spec.shareProcessNamespace` 是否和 sandbox 共享 PID namespace。

#### status
phase

* Pending: 没有容器启动，可能还未绑定到 node 等待调度，或者在拉镜像
* Running: 所有容器已经启动，至少有一个容器在运行或在重启
* Succeed: 所有容器成功退出，而不重启
* Failed: 所有容器已终止，但至少有一个退出码不为0
* Unknown: 不能获取状态，通常是通信问题

condition

* PodScheduled: 已进行调度
* Ready: 可以提供服务，加入 service 负载平衡
* Initialized: init 容器成功完成
* Unschedulable: 未能成功调度
* ContainersReady: 所有容器已 ready

### Deployment
部署策略

[![img](https://raw.githubusercontent.com/ContainerSolutions/k8s-deployment-strategies/master/decision-diagram.png)](https://container-solutions.com/kubernetes-deployment-strategies/)

* 重建 recreate: 先删掉旧的版本，再发布新的版本。`.spec.strategy.type==Recreate`
* 滚动更新 ramped: 一个接一个更新，更新时新旧版本共存。 `.spec.strategy.type==RollingUpdate`（默认）
  * maxSurge: 允许除 desired 外创建多余的新 pod 数目，默认 %25 desired。
  * maxUnavailable: 最多不可用的 pod 数目，默认 %25 desired。
* 蓝/绿 blue/green: 先发布新版本，再将流量从旧版本切到新版本，最后删掉旧版本。通过 service.selector 更新不同版本切换。
* 金丝雀 canary: 先发布新版本，再将部分流量切到新版本，最后再切全量。使用两个 deployment，设置不同的 replicas 比例。
* a/b 测试: 类似 canary，但以更精确的方式（比如 HTTP 头）针对发布。利用 Istio, Nginx 等控制。
* 阴影 shadow: 新版本和旧版本共存，但只接收少部分流量，且不影响回应。可以通过 Istio 复制请求等方式。

用 `spec.revisionHistoryLimit` 控制历史版本数 （旧`ReplicaSet`数目）。

### StatefulSet
指定 `serviceName` 为对应的 Headless Service。

* 利用 Headless Service，每个 pod 的 DNS 域名：`<statefulset-name>-<seq>.<svc-name>.<namespace>.svc.cluster.local`
* 如果申请了持久存储，名字也一一和 pod 对应：`<pvc-name>-<statefulset-name>-<seq>`
* 创建和删除 pod 时，严格按照编号顺序（删除反序）。

更新策略：`.spec.updateStrategy.type`

* `rollingUpdate`（默认）: 滚动更新，按照删除的顺序更新，每次更新一个 pod。
  * `partition`: 分区，只有序号大于等于其值的才会更新。
* `onDelete`: 不更新，必须手动删除老 pod，让 controller 创建新 pod 来更新。

### DaemonSet
通过 `nodeAffinity` 和 `toleration` 保证每个 node 上运行一个 pod 实例。

DaemonSet 和 StatefulSet 都通过 `ControllerRevision`（保存了历史版本内容） 进行版本控制，而 Deployment 是直接通过 `ReplicaSet`。

### Job & CronJob
Job 通过 controller 生成的 `controller-uid=` 作为 selector，控制选中的 pod，是为了保证一个 Job 和其控制 pod 的唯一对应关系（新 Job 不应再控制旧的正在运行或已完成的 pod）。相比 Deployment/STS/DS 使用用户定义的 label 作为 selector，因为是 RLS 不要求这种唯一对应关系。

restartPolicy 在 Job 对象里只允许被设置为 Never/OnFailure, 在 Deployment 里只能是 Always。

* 当 Nerver 时，用 `spec.backoffLimit` 控制失败次数（重新创建 pod 次数）。
* `spec.activeDeadlineSeconds` 控制运行超时，也可以防止 OnFailure 时一直重启。

并发控制

* 不需要并发：不设置 `.spec.completions` 和 `.spec.parallelism`，默认都为1。
* 固定成功完成数：设置 `.spec.completions`，不设置 `.spec.parallelism` （默认为1，可设大于1）。
* 工作队列：不设置 `.spec.completions`，只设置 `.spec.parallelism`。完成数不能确定，只要有一个 pod 成功，Job 就算成功（其他 pod 应该自动退出）。

CronJob 是定时任务，控制 Job。
`spec.concurrencyPolicy`: Allow Job可以同时存在；Forbid 跳过，Job不会被创建；Replace 新 Job 替换旧 Job。

### Service
Headless Service: `spec.ClusterIP: None`

* 没有 VIP，DNS 服务名直接解析为选中的多个 pod IP 的 A 记录。
* 每个选中的 pod 有唯一网络标识：`<pod-name>.<svc-name>.<namespace>.svc.cluster.local`

[Source ip 问题](https://kubernetes.io/docs/tutorials/services/source-ip/)
kube-proxy 使用 iptable 的情况，通过下面方式访问

* ClusterIP: 不做 SNAT
* NodePort: SNAT 成对应 node 的地址，可以将`service.spec.externalTrafficPolicy`设为`Local`（默认`Cluster`）只发本地node上的endpoint来避免（要求node上有对应endpoint），但因为不像`Cluster`那样对所有endpoint全局轮发，可能导致负载不均衡。
* LoadBalancer: 情况和 NodePort 类似，如果`service.spec.externalTrafficPolicy`设为`Local`，LB只会转发流量到有对应endpoint的node上（通过health check `service.spec.healthCheckNodePort`）。

### PersistentVolume & PersistentVolumeClaim & StorageClass
PVC 和 PV 一一绑定：PV 的 spec （容量等）满足 PVC 的要求；storage class 一致（StorageClass 可以设置缺省值`storageclass.kubernetes.io/is-default-class`）。

provisioning:
静态 provisioning:（StorageClass provisioner=kubernetes.io/no-provisioner）手动创建 PV。
动态 provisioning:（StorageClass 指定了具体的 provisoner）: PVC 指定 storage class 名字，provisoner 根据 PVC 的要求及 StorageClass 的属性，创建出 PVC 对应的 PV。
绑定执行：kube-controller-manager `PersistentVolumeController`

attaching:
为 node 挂载磁盘（一般为块设备，其他比如分布式文件系统 NFS，CephFS 并不需要）。
执行：kube-controller-manager (`AttachDetachController`调用具体 volume plugin，如rbd, csi...的`Attacher.Attach`)

mounting:
格式化磁盘（或直接用远程存储）并 mount 到 pod 的 volume 目录 `/var/lib/kubelet/pods/<Pod-ID>/volumes/kubernetes.io~<Volume-Type>/<Volume-Name>`。
执行：kubelet (`VolumeManager`调用具体 volume plugin 的 `DeviceMounter.MountDevice`格式化等准备工作和 `Mounter.SetUp`具体挂载)

#### local PV
```
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```
不支持动态 provisioning: 需要手动创建 PV（指定节点信息），可以利用 [local-static-provisioner](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner)自动创建 PV（当对应绑定的 PVC 删除后，对应 PV 会清除并重建）。
延迟绑定: PVC 和 PV 不立即绑定，等到第一个使用 PVC 的 Pod 调度的时候，由调度器综合考虑 PV 节点情况再绑定。（删除 Pod 后，PVC 和 PV 还是绑定状态，若不删除，下次 Pod 还是会调度到这个节点上）

## Misc.
### [QoS](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-resource-management.md)
* Guaranteed: Pod 里每个容器 CPU/Memory 的 request 和 limit 都必须设置且值一样。
* Burstable: Pod 里至少有一个容器设置了 CPU/Memory 的 request。
* BestEffort: Pod 里所有容器都没有设置 CPU/Memory 的 request 和 limit。
（如果设置 limit，而没有设置对应的 request，默认和 limit 一样）
当发生 Pod Eviction 的时候按照 BestEffort, Burstable, Guaranteed 的优先级别进行删除。

cgroup 设置
```
[cpu/memory]/kubepods/[besteffort/burstable]/pod{uid}/containersxxx/
                     /pod{uid}/containersxxx/   # guaranteed
```
cpu limit: cpu.cfs_quota_us, cpu_period_us
cpu request: cpu.shares
memory limit: memory.limit_in_bytes
memory request: 根据`experimental-qos-reserved=memory`参数在 [besteffort/burstable] cgroup 进行预留控制

### Initializer
1. `InitializerConfiguration` 配置所用 Initializer 的名字及需匹配的资源。
1. 资源的 `metadata.initializers.pending` 中增加 Initializer 名字，并不被创建等待初始化。
1. Initializer (自定义 controller) 根据配置初始化资源，并删除 `metadata.initializers.pending` 对应项。
1. 资源被创建。

### RBAC
* `Role`/`ClusterRole`: 角色，及具体资源的权限规则
* Subject: 作用对象
* `RoleBinding`/`ClusterRoleBinding`: Role 和 Subject 的绑定关系

Subject:

* `ServiceAccount`, pod 指定后认证信息(对应`kubernetes.io/service-account-token`类型的Secret)会挂载在 `/var/run/secrets/kubernetes.io/serviceaccount` 目录。
* `User`: `system:serviceaccount:<ServiceAccount-Name>` （SA对应）
* `Group`: `system:serviceaccounts:<Namespace-Name>` （内置，对应Namespace里面的所有SA）

### Monitoring
#### metrics
node: 通过 [node exporter](https://github.com/prometheus/node_exporter)
核心组件：通过 `/metrics` API
core metrics: pod, container, node等: 通过 [MetricsServer](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/metrics-server.md) (`/stats/summary` from kubelet cAdvisor)

#### ServiceMonitor
1. 应用暴露指标 `http.Handle("/metrics", promhttp.Handler())`
1. Service 暴露端口
1. 创建 `ServiceMonitor`

```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: myapp
  namespace: default
spec:
  endpoints:
  - interval: 30s
    port: http-metrics
  jobLabel: app
  namespaceSelector:
    matchNames:
    - default
  selector:
    matchLabels:
      app: myapp
```

#### MetricsServer
利用 [apiserver aggregation](https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/)
将 [prometheus adapter](https://github.com/DirectXMan12/k8s-prometheus-adapter) 作为 MetricsServer
```
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
spec:
  group: metrics.k8s.io
  groupPriorityMinimum: 100
  insecureSkipTLSVerify: true
  service:
    name: prometheus-adapter
    namespace: monitoring
  version: v1beta1
  versionPriority: 100
```

## Utilities
### Leader Election
[![img](https://cdn-images-1.medium.com/max/800/0*1bwwLXjQrLWL67RQ)](https://medium.com/@dominik.tornow/kubernetes-high-availability-d2c9cbbdd864)

`k8s.io/client-go/tools/leaderelection`

配置：

* `LeaseDuration` 租期
* `RenewDeadline` 续租间隔

`tryAcquireOrRenew` 获得锁的条件（或）：

* 锁资源还没创建，创建并声明获得
* Leader Lease 过期了 （本地当前时间已经超过了最后一次在本地观察到 record 变化（即刷新过）的时间+本地配置的租期）
* 现 Leader 就是自己

一般用 `endpoint`/`configmap` 表示资源锁对象：
相关信息 `LeaderElectionRecord`（只有`HolderIdentity`被用来判断自己是否是 leader，其他都是记录信息） 存在 `control-plane.alpha.kubernetes.io/leader` annotation 里面。

```
type LeaderElectionRecord struct {
	HolderIdentity       string      `json:"holderIdentity"`       // hostname + uuid
	LeaseDurationSeconds int         `json:"leaseDurationSeconds"` // 租期
	AcquireTime          metav1.Time `json:"acquireTime"`          // 获取时间
	RenewTime            metav1.Time `json:"renewTime"`            // 刷新时间
	LeaderTransitions    int         `json:"leaderTransitions"`    // 转换次数
}
```

不能保证只有一个 leader (fencing) , 如果出现2个(split brain)，由 controller 自己靠调谐 (reconciliation) 来解决冲突。
可以容忍绝对时钟偏移值（因为计算都用的本地单调时钟），但会受到时钟偏移率的影响。

</xmp>
<script src="../js/strapdown-zeta.js"></script>
</html>
