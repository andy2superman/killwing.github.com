<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>Kubernetes</title>
<xmp theme="united" style="display:none;" toc="true">
https://kubernetes.io

## 安装部署
### kubeadm
#### 安装
所有 node 上执行。
```
apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
```

最新稳定版：`https://dl.k8s.io/release/stable-1.txt` (v1.13.2)
镜像：`kubeadm config images list`
```
k8s.gcr.io/kube-apiserver:v1.13.2
k8s.gcr.io/kube-controller-manager:v1.13.2
k8s.gcr.io/kube-scheduler:v1.13.2
k8s.gcr.io/kube-proxy:v1.13.2
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
k8s.gcr.io/coredns:1.2.6
```

手动拉镜像并重命名
```
#!/bin/bash
images=`kubeadm config images list`
for i in $images; do
	image=registry.cn-hangzhou.aliyuncs.com/google_containers/${i#k8s.gcr.io/}
	docker pull $image
	docker tag $image $i
done
```

#### init master
`kubeadm init --config config.yaml`
默认配置：`kubeadm config print init/join-defaults`
详细说明：https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta1

config.yaml
```
apiVersion: kubeadm.k8s.io/v1beta1
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.212.107
  bindPort: 6443
nodeRegistration:
  kubeletExtraArgs:
    dynamic-config-dir: /var/lib/kubelet/dynamic-config
---
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
apiServer:
  extraArgs:
    enable-admission-plugins: NodeRestriction,ExtendedResourceToleration
networking:
  podSubnet: "10.100.0.0/16" # 需与 calico 配置一致，且不与 host 网络冲突
imageRepository: "registry.cn-hangzhou.aliyuncs.com/google_containers"
kubernetesVersion: "v1.13.2"
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
featureGates:
    DynamicKubeletConfig: true
```

网络插件
```
kubectl apply -f https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
```

#### join node
`kubeadm token create --print-join-command`
`kubeadm join <ip>:<port> --token <token> --discovery-token-ca-cert-hash <hash>`

#### 删除 node
```
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>
kubeadm reset
```

reset iptable:
`iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X`

#### 更新配置
组件：直接更新 `/etc/kubernetes/manifests` 配置
kubelet: 使用[configMap](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), 必须一开始加上 `--dynamic-config-dir` 和开启 `DynamicKubeletConfig` feature gate （默认true）。

#### 相关配置
`/etc/kubernetes/*.conf` 各组件访问 APIServer 鉴权文件。
`/etc/kubernetes/pki/` ca公钥私钥。
`/etc/kubernetes/manifests` 各组件静态 Pod。
`/var/lib/kubelet/kubeadm-flags.env` kubeadm 为 kubelet 设置的 `KUBELET_EXTRA_ARGS`。
`/var/lib/kubelet/config.yaml` kubelet 本地 configMap 配置文件。
`/etc/default/kubelet` kubelet `KUBELET_EXTRA_ARGS=` 配置。
`/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` kubelet systemd 配置，启动命令行。

### 其它插件
[nvidia-device-plugin](https://github.com/NVIDIA/k8s-device-plugin)
`kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml`

[dashboard](https://github.com/kubernetes/dashboard)
`kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml`

[rook](https://github.com/rook/rook)
```
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
```

[kube-prometheus](https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus)

[logging](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch)

### Tips
kubelet alias 自动补全(Linux)：
`source <(kubectl completion bash | sed 's/kubectl/kc/g')`

### 验证
#### [Node Conformance Test](https://kubernetes.io/docs/setup/node-conformance/)
在 node 加入集群之前验证。
run: `test/e2e_node/conformance/run_test.sh`

#### Cluster
validate cluster: `KUBECTL_PATH=$(which kubectl) NUM_NODES=1 KUBERNETES_PROVIDER=local cluster/validate-cluster.sh`

#### 其他
https://github.com/heptio/sonobuoy
https://github.com/bloomberg/powerfulseal

## Networking
Networking Model:

* all containers can communicate with all other containers without NAT
* all nodes can communicate with all containers (and vice-versa) without NAT
* the IP that a container sees itself as is the same IP that others see it as

### 网络方案
#### Overlay (UDP/VXLAN)
[![img](https://cdn-images-1.medium.com/max/1600/1*JqSLd3cPv14BWDtE7YEcRA.png)](https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c)

[flannel](https://github.com/coreos/flannel)

#### Routing (Host-GW)
![img](https://feisky.gitbooks.io/sdn/container/calico/calico.png)

[calico](https://www.projectcalico.org/)

组件：

[bird](http://bird.network.cz/): 使用 [BGP](https://www.projectcalico.org/why-bgp/) 协议，在节点之间共享路由信息。
[felix](https://github.com/projectcalico/felix): 监听 etcd 中的网络配置，根据配置更新节点的路由和 iptables。
[confd](https://github.com/kelseyhightower/confd): 监听 etcd 的数据，实时更新 bird 的配置文件，并重新启动 bird 进程以加载最新配置。
[controller](https://github.com/projectcalico/kube-controllers): 使用独立的 etcd 需要，使其中的数据和 k8s 同步。
[cni](https://github.com/projectcalico/cni-plugin): k8s 网络管理插件（同时管理 `WorkloadEndpoint` 资源）（纯 docker 使用 [libnetwork](https://github.com/projectcalico/libnetwork-plugin)）。

 * calico 主插件
 * calico-ipam ip分配
 * `/etc/cni/net.d/10-calico.conflist`
```
{
    "name": "k8s-pod-network",
    "cniVersion": "0.3.0",
    "plugins": [
      {
        "type": "calico", # 插件名字
        "log_level": "info",
        "datastore_type": "kubernetes",
        "nodename": "node1",
        "mtu": 1440,
        "ipam": {
          "type": "host-local", # 使用本地 ipam 插件 而不是 calico-ipam
          "subnet": "usePodCidr"
        },
        "policy": {
            "type": "k8s"
        },
        "kubernetes": {
            "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
        }
      },
      {
        "type": "portmap",
        "snat": true,
        "capabilities": {"portMappings": true}
      }
    ]
}
```

资源：

* HostEndpoint: 对应 node 上的网络接口。
* WorloadEndpoint: 对应 pod 的 veth 网络接口（`cali*`），区分 namespace。
* Profile: 应用于某个单独的 endpoint，每个 endpoint 可以有多个 profile。
* NetworkPolicy: 通过 label selector 应用于一组 endpoint (label 从 pod 复制)，对应于 k8s 的 `NetworkPolicy`。

### [CNI](https://github.com/containernetworking/cni)
[spec](https://github.com/containernetworking/cni/blob/master/SPEC.md) / [k8s cni](https://github.com/kubernetes/community/blob/master/contributors/devel/kubelet-cri-networking.md)

功能：

* 在容器网络命名空间中配置网络接口。
* 在host上配置网络。
* 给容器中的网络接口分配IP地址以及路由。

[标准基础插件](https://github.com/containernetworking/plugins)：

* main: 创建网络设备
* IPAM: IP地址分配
* meta: 其他(内置flannel)

plugin位置：`/opt/cni/bin`
配置(Network Configuration Lists)：`/etc/cni/net.d` 只会加载按字母顺序第一个，如 `10-flannel.conflist`

执行环境：由 CRI (如dockershim) 调用: [`RunPodSandbox` -> `SetUpPod`](https://github.com/kubernetes/kubernetes/blob/4dc40aabfb9af68913986a2caf87906312c9f4ea/pkg/kubelet/dockershim/docker_sandbox.go#L177)

执行参数：

* 环境变量：`CNI_*`
  * `CNI_COMMAND`: `ADD`/`DEL`/`CHECK`/`VERSION`
  * `CNI_CONTAINERID`: 容器ID
  * `CNI_NETNS`: 网络命名空间路径, `/proc/<container_pid>/ns/net`
  * `CNI_IFNAME`: 容器内网络接口名字
  * `CNI_ARGS`: KV形式的自定义参数
  * `CNI_PATH`: plugin搜索路径
* stdin: `Network Configuration` 调用中可能会被修改
```
{
    "cniVersion": "0.4.0",
    "name": "dbnet",
    "type": "bridge",
    // type (plugin) specific
    "bridge": "cni0",
    "ipam": {
      "type": "host-local",
      // ipam specific
      "subnet": "10.1.0.0/16",
      "gateway": "10.1.0.1"
    },
    "dns": {
      "nameservers": [ "10.1.0.1" ]
    }
}
```
`delegate` 字段（内容和 `Network Configuration` 相同）可以用于委派其他插件完成。

执行结果(`ADD`)：

* result code: 1/2/3/11
* stdout: `Result` (`IPAM`的结果没有interfaces)
```
{
    "cniVersion": "0.4.0",
    "interfaces": [...],
    "ips": [...],
    "routes": [...],
    "dns": [...]
}
```
失败结果：
```
{
    "cniVersion": "0.4.0",
    "code": <numeric-error-code>,
    "msg": <short-error-message>,
    "details": <long-error-message> (optional)
}
```

整体List执行：
`ADD`按照`Network Configuration Lists`配置的plugins顺序执行，`DEL`反序执行。
```
{
    "cniVersion": "0.4.0",
    "name": "...",
    "disableCheck": false,
    "plugins": [<Network Configuration>, ...]
}
```

对于 `ADD`, 把结果 `Result` 作为`Network Configuration` 的一个新增字段 `prevResult` 传给下一个plugin。后面的 plugin 除非需要修改或覆盖结果，都应该把收到的 `prevResult` 原样输出给下一个。
如果 `ADD` 中有一个 plugin 失败，执行 `DEL` (全部 plugins) 作为错误处理。
`DEL` 输入也包含最近 `ADD` 的 `Result` 作为 `prevResult`。

## Storage
### [CSI](https://kubernetes-csi.github.io/)
[spec](https://github.com/container-storage-interface/spec/blob/master/spec.md) / [k8s csi](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md)

[![img](https://cdn-images-1.medium.com/max/1000/1*oMgMPjx0obXKlaItZOkRfA.png)](https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b)

external components (从 kubelet 剥离，由 k8s 维护):

* [node driver registrar](https://github.com/kubernetes-csi/node-driver-registrar): [registering] 向 kubelet 注册 CSI 驱动（利用 deivce plugin 的注册机制），触发 CSI 端点的 `GetPluginInfo/GetPluginCapabilities` 方法。安装驱动可执行文件。以及健康检测。
* [cluster driver registrar](https://github.com/kubernetes-csi/cluster-driver-registrar): 为驱动创建 `CSIDriver` 对象。自定义一些行为（attachRequired/podInfoOnMountVersion）。（非必要）
* [external provisioner](https://github.com/kubernetes-csi/external-provisioner): [provisioning] 监听 `PersistentVolumeClaim` 对象，触发 CSI 端点的 `CreateVolume/DeleteVolume` 方法，创建对应的 `PersistentVolume`。`StorageClass.provisioner` 要和 CSI 端点 `GetPluginInfo` 返回的一致。
* [external attacher](https://github.com/kubernetes-csi/external-attacher): [attaching] 监听 `VolumeAttachment` 对象（当 pod 调度到 node 时，由 kubelet 创建），触发 CSI 端点的 `ControllerPublish/Unpublish` 方法，更新 `VolumeAttachment` 状态为 `Attached`。

挂载的一般流程：(registering) -> provisioning -> attaching -> mounting
[mounting] kubelet 监听 `VolumeAttachment` 的状态，当为 `Attached` 时，触发 CSI 端点的 `NodePublishVolume/NodeUnpublishVolume` 方法进行实际的挂载。

csi driver / plugin （第三方提供）：

* CSI Identity: 提供驱动的标识和基本信息。

```
service Identity {
  rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {}
  rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {}
  rpc Probe (ProbeRequest) returns (ProbeResponse) {}
}
```

* CSI Controller: 提供卷的基本服务：创建，删除，attach/dettach (publish/unpublish，指将设备挂到node上，一般只有块存储需要), snapshoting 等。（volume contoller逻辑，不需要在node上操作，部署在master节点上）。

```
service Controller {
  rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {}
  rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {}
  rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {}
  rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {}
  rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest) returns (ValidateVolumeCapabilitiesResponse) {}
  rpc ListVolumes (ListVolumesRequest) returns (ListVolumesResponse) {}
  rpc GetCapacity (GetCapacityRequest) returns (GetCapacityResponse) {}
  rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest) returns (ControllerGetCapabilitiesResponse) {}
  rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {}
  rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {}
  rpc ListSnapshots (ListSnapshotsRequest) returns (ListSnapshotsResponse) {}
  rpc ControllerExpandVolume (ControllerExpandVolumeRequest) returns (ControllerExpandVolumeResponse) {}
}
```

* CSI Node: 在node上执行的具体操作（由 kubelet 直接调用），如格式化以及 mount/unmount 存储路径到 pod volume 的 mount point 上。

```
service Node {
  rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {}
  rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {}
  rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {}
  rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {}
  rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {}
  rpc NodeExpandVolume(NodeExpandVolumeRequest) returns (NodeExpandVolumeResponse) {}
  rpc NodeGetCapabilities (NodeGetCapabilitiesRequest) returns (NodeGetCapabilitiesResponse) {}
  rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {}
}
```

部署：
![img](https://raw.githubusercontent.com/kubernetes/community/master/contributors/design-proposals/storage/container-storage-interface_diagram1.png)

(StatefulSet/Deployment) external-attacher/provisioner 通过 emptyDir 里的由 CSI Driver 创建的 socket 和其通信。
(DaemonSet) node-driver-registrar 通过 hostpath `/var/lib/kubelet/plugins_registry/[SanitizedCSIDriverName]-reg.sock` 和 CSI Driver 通信。
kubelet 通过 hostpath `/var/lib/kubelet/plugins/[SanitizedCSIDriverName]/csi.sock` 和 CSI Driver 通信。
`/var/lib/kubelet/pods/` 必须以 `mountPropagation: "Bidirectional"` 挂载进 CSI Driver，这样 CSI Driver 里的挂载在 host 上及用户容器里才能看到。


## Runtime
### [CRI](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md)

```
service RuntimeService {
    rpc Version(VersionRequest) returns (VersionResponse) {}
    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}
    rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {}
    rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {}
    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}
    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}
    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}
    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}
    rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {}
    rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {}
    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}
    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}
    rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {}
    rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {}
    rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {}
    rpc Exec(ExecRequest) returns (ExecResponse) {}
    rpc Attach(AttachRequest) returns (AttachResponse) {}
    rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {}
    rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) {}
    rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) {}
    rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {}
    rpc Status(StatusRequest) returns (StatusResponse) {}
}

ervice ImageService {
    rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {}
    rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {}
    rpc PullImage(PullImageRequest) returns (PullImageResponse) {}
    rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {}
    rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {}
}
```

```
[kubelet] -- (CRI) -- [dockershim] -- [docker daemon] -- [containerd] -- (OCI) -- [runc]
                   -- [cri-containerd] ------------------^               ^
                   -- [cri-o] -------------------------------------------'
```

默认 kubelet (`--container-runtime=docker`) 通过 `--container-runtime-endpoint=/var/run/dockershim.sock`（image 默认和 runtime 相同） 和实现了 CRI 的 dockershim GRPC server 通信。
若使用 cri-o, 则配置 `--container-runtime=remote` 及 `--container-runtime-endpoint=/var/run/crio/crio.sock`。

在 docker runtime 下，Exec/Attach/PortForward 会被重定向到 kubelet 的 `/cri/` 路径下，由 streaming server 处理（可以是独立 HTTP server，实际直接注册 handler 到 kubelet 中），最终由实现了 `Runtime` 接口的 `streamingRuntime` 执行。
```
type Runtime interface {
	Exec(containerID string, cmd []string, in io.Reader, out, err io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize) error
	Attach(containerID string, in io.Reader, out, err io.WriteCloser, tty bool, resize <-chan remotecommand.TerminalSize) error
	PortForward(podSandboxID string, port int32, stream io.ReadWriteCloser) error
}
```
即 `apiserver -(exec cmd)-> kubelet -(cri.Exec)-> dockershim -(redirect /cri/exec/)-> kubelet -(runtime.Exec)-> streamingServer -(exec container)-> container`


`kubeGenericRuntimeManager.SyncPod`

1. 创建并启动 sandbox 容器，使用 pause 镜像，调用 CNI 插件配置网络。
1. 按顺序创建并启动 init 容器。
1. 创建并启动普通容器。容器共享 sandbox 的 Network/Ipc namespace，UTS 根据 `podSpec.hostNetwork` 是否和 node 共享，Pid 根据 `podSpec.shareProcessNamespace` 是否和 sandbox 共享。

### [cri-o](https://cri-o.io/)
* kubelet 运行 pod，pod 里的容器共享同样的 IPC/NET/PID namespace 和 cgroup。
* kubelet 通过 CRI 接口向 CRI-O daemon 发送运行 pod 的请求。
* CRI-O 使用 `containers/image` 库从 registry 拉取镜像。
* CRI-O 使用 `containers/storage` 库将镜像解压成 rootfs，存放于 COW 文件系统里。
* 当容器的 rootfs 创建后，CRI-O 使用 OCI 生成工具生成一份描述如何运行容器的 OCI 运行时 json 配置。
* CRI-O 启动 OCI 兼容的运行时（默认 [runc](https://github.com/opencontainers/runc)）根据配置来运行容器进程。
* 每个容器都被一个独立的 conmon 进程监控，监控进程还持有容器进程(PID1)的 pty 终端，处理日志和记录容器进程的退出码。
* Pod 的网络由 CNI 设置，CRI-O 可以使用任何 CNI 插件。

## Components
### [Scheduler](https://github.com/kubernetes/community/blob/master/contributors/devel/scheduler.md)
### [Controller](https://github.com/kubernetes/community/blob/master/contributors/devel/controllers.md)
#### [client-go](https://github.com/kubernetes/client-go/)
[![img](https://raw.githubusercontent.com/kubernetes/sample-controller/master/docs/images/client-go-controller-interaction.jpeg)](https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md)

### [Device Plugin](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md)

![img](https://raw.githubusercontent.com/kubernetes/community/master/contributors/design-proposals/resource-management/device-plugin-overview.png)
DP 通过 Daemonset 部署在每个 node 上。

DP 通过 `/var/lib/kubelet/device-plugins/kubelet.sock` 向 kubelet 注册:

 * DP 的版本
 * 端点名字，在 `/var/lib/kubelet/device-plugins/` 下
 * 资源名字

```
service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}
```

kubelet 通过 `/var/lib/kubelet/device-plugins/{注册的端点名字}` 向 DP 请求：
```
service DevicePlugin {
	// returns a stream of []Device
	rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}
	rpc Allocate(AllocateRequest) returns (AllocateResponse) {}
}
```

`ListAndWatch`: 返回一组设备，包括其 ID 和健康状况。
`Allocate`: 请求一组设备（请求哪些空闲的设备是 kubelet 决定的），返回其在容器内的运行时设置。运行时设置包括: 一组环境变量，需要从 node 上 mount 的一组目录（通常包括驱动和工具文件），需要 mount 的具体设备文件。


DP 可以通过 `/var/lib/kubelet/pod-resources/kubelet.sock` 向 kubelet 查询设备资源在 node 上的分配情况。
```
// PodResources is a service provided by the kubelet that provides information about the
// node resources consumed by pods and containers on the node
service PodResources {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
}
```

## Resources
### [Service](https://kubernetes.io/docs/concepts/services-networking/service/)
[Source ip 问题](https://kubernetes.io/docs/tutorials/services/source-ip/)
kube-proxy 使用 iptable 的情况，通过下面方式访问

* ClusterIP: 不做 SNAT
* NodePort: SNAT 成对应 node 的地址，可以将`service.spec.externalTrafficPolicy`设为`Local`（默认`Cluster`）只发本地node上的endpoint来避免（要求node上有对应endpoint），但因为不像`Cluster`那样对所有endpoint全局轮发，可能导致负载不均衡。
* LoadBalancer: 情况和 NodePort 类似，如果`service.spec.externalTrafficPolicy`设为`Local`，LB只会转发流量到有对应endpoint的node上（通过health check `service.spec.healthCheckNodePort`）。

## Misc.
### [QoS](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-resource-management.md)
* Guaranteed: Pod 里每个容器 CPU/Memory 的 request 和 limit 都必须设置且值一样。
* Burstable: Pod 里至少有一个容器设置了 CPU/Memory 的 request。
* BestEffort: Pod 里所有容器都没有设置 CPU/Memory 的 request 和 limit。

（如果设置 limit，而没有设置对应的 request，默认和 limit 一样）

cgroup 设置
```
[cpu/memory]/kubepods/[besteffort/burstable]/pod{uid}/containersxxx/
                     /pod{uid}/containersxxx/   # guaranteed
```
cpu limit: cpu.cfs_quota_us, cpu_period_us
cpu request: cpu.shares
memory limit: memory.limit_in_bytes
memory request: 根据`experimental-qos-reserved=memory`参数在 [besteffort/burstable] cgroup 进行预留控制

## Utilities
### Leader Election
[![img](https://cdn-images-1.medium.com/max/800/0*1bwwLXjQrLWL67RQ)](https://medium.com/@dominik.tornow/kubernetes-high-availability-d2c9cbbdd864)

`k8s.io/client-go/tools/leaderelection`

配置：

* `LeaseDuration` 租期
* `RenewDeadline` 续租间隔

`tryAcquireOrRenew` 获得锁的条件（或）：

* 锁资源还没创建，创建并声明获得
* Leader Lease 过期了 （本地当前时间已经超过了最后一次在本地观察到 record 变化（即刷新过）的时间+本地配置的租期）
* 现 Leader 就是自己

一般用 `endpoint`/`configmap` 表示资源锁对象：
相关信息 `LeaderElectionRecord`（只有`HolderIdentity`被用来判断自己是否是 leader，其他都是记录信息） 存在 `control-plane.alpha.kubernetes.io/leader` annotation 里面。

```
type LeaderElectionRecord struct {
	HolderIdentity       string      `json:"holderIdentity"`       // hostname + uuid
	LeaseDurationSeconds int         `json:"leaseDurationSeconds"` // 租期
	AcquireTime          metav1.Time `json:"acquireTime"`          // 获取时间
	RenewTime            metav1.Time `json:"renewTime"`            // 刷新时间
	LeaderTransitions    int         `json:"leaderTransitions"`    // 转换次数
}
```

不能保证只有一个 leader (fencing) , 如果出现2个(split brain)，由 controller 自己靠调谐 (reconciliation) 来解决冲突。
可以容忍绝对时钟偏移值（因为计算都用的本地单调时钟），但会受到时钟偏移率的影响。

</xmp>
<script src="../js/strapdown-zeta.js"></script>
</html>
