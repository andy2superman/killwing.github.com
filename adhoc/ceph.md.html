<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>Ceph</title>
<xmp theme="united" style="display:none;">
## Install
安装 ceph-deploy
```
apt-get update && apt-get install ceph-deploy
export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/debian-nautilus # v14
export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc
```

保证 node 之间 ssh 互通，创建 cluster，三节点quorum
```
ceph-deploy new node1 node2 node3
```

生成配置，并修改：
```
[global]
fsid = 26247a53-0175-4ad4-b416-0a0e13431054
mon_initial_members = node1, node2, node3
mon_host = 10.255.1.41,10.255.1.36,10.255.1.128
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx

osd pool default size = 2

# https://github.com/ceph/ceph-csi/issues/486
[mgr]
client mount uid = 0
client mount gid = 0

[mds]
mds cache memory limit = 4294967296
```

[config sample](https://github.com/ceph/ceph/blob/master/src/sample.ceph.conf)

若要更新配置：
```
ceph-deploy --overwrite-conf config push node1 node2 node3
```

安装到指定节点
```
ceph-deploy install --release nautilus node1 node2 node3
```

创建 MON
```
ceph-deploy mon create-initial # 在初始节点上创建 mon
ceph-deploy admin node1 node2 node3 # 拷贝鉴权配置文件，成为admin操作节点
```

增加 OSD
```
ceph-deploy disk zap node1 /dev/sdb # 准备磁盘，删除分区等
ceph-deploy osd create node1 --data /dev/sdb # 默认 bluestore, Block Data & WAL & DB 在同一块盘
```

* osd 基本元信息在 `/var/lib/ceph/osd/ceph-{idx}`
* 如果 Block 和 DB 分开放，DB 大小不应小于 Block 的 4%

增加 manager，active/standby
```
ceph-deploy mgr create node1 node2
```

创建 pool
```
ceph osd pool create <poolname> <pg-num> {pgp-num} {replicated|erasure} {erasure_code_profile} {rule} {expected-num-objects}
ceph osd pool set <poolname> size 3 # 改为3副本
ceph osd pool application enable <poolname> rbd # for rbd
ceph osd pool ls detail
```
[pg 计算](https://ceph.com/pgcalc/)

创建 mds (for cephfs)
```
ceph-deploy mds create node1 node2 node3
ceph osd pool create cephfs-data <pg_num>
ceph osd pool create cephfs-metadata <pg_num>
ceph fs new <fs-name> cephfs-metadata cephfs-data
ceph fs set <fs_name> max_mds 2 # 设为2个 active mds
ceph fs status
```

## LVM
* PE: 物理区，PV 中分配的最小单元（默认4M）
* PV: 物理卷，对应物理硬盘或分区
  * pvcreate, pvs, pvdisplay, pvremove
* VG: 卷组，包含一个或多个 PV，所有 PV 的 PE 大小相同
  * vgcreate, vgs, vgdisplay, vgextend, vgreduce, vgremove
* LV: 逻辑卷，在 VG 里面分割，大小可动态改变（为 PE 整数倍）
  * lvcreate, lvs, lvdisplay, lvextend, lvreduce, lvremove
  * device 路径在 /dev/{vg}/{lv}，后可以格式化挂载使用
  * 分 [linear 和 striped](https://sysadmincasts.com/episodes/27-lvm-linear-vs-striped-logical-volumes) 模式，`lsdisplay -m` 查看

## Crush Map
```
# begin crush map
tunable choose_local_tries 0
tunable choose_local_fallback_tries 0
tunable choose_total_tries 50
tunable chooseleaf_descend_once 1
tunable chooseleaf_vary_r 1
tunable chooseleaf_stable 1
tunable straw_calc_version 1
tunable allowed_bucket_algs 54

# devices 对应每个 osd
device 0 osd.0 class ssd
device 1 osd.1 class ssd
device 2 osd.2 class hdd
device 3 osd.3 class hdd

# types 桶类型, 表示 topo 位置
type 0 osd
type 1 host
type 2 chassis
type 3 rack
type 4 row
type 5 pdu
type 6 pod
type 7 room
type 8 datacenter
type 9 region
type 10 root

# buckets 桶
host node1 {
        id -3
        id -4 class ssd
        id -7 class hdd
        alg straw2
        hash 0  # rjenkins1
        item osd.0 weight 1 # 权重，一般 1T 为 1
        item osd.2 weight 1
}
host node2 {
        id -5
        id -6 class ssd
        id -8 class hdd
        alg straw2
        hash 0  # rjenkins1
        item osd.1 weight 1
        item osd.3 weight 1
}

root default {
        id -1
        id -2 class ssd
        id -9 class hdd
        alg straw2
        hash 0  # rjenkins1
        item node1 weight 2
        item node2 weight 2
}

# rules
rule replicated_rule {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default # 起始桶
        step chooseleaf firstn 0 type host # 0 表示选择 pool-num-replicas 个桶
        step emit # 输出当前值，清空栈
}

rule ssd {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default class ssd # 只选择匹配 ssd 的
        step chooseleaf firstn 0 type host
        step emit
}

# end crush map
```

rule placement:
```
tack(a)
choose
    choose firstn {num} type {bucket-type} # 选出 num 个指定类型的桶
    chooseleaf firstn {num} type {bucket-type} # 先选出 num 个指定类型的桶，然后在桶中选择一个叶子结点
        if {num} == 0, choose pool-num-replicas buckets (all available).
        if {num} > 0 && < pool-num-replicas, choose that many buckets.
        if {num} < 0, it means pool-num-replicas - {num}.
emit
```
查看树：`ceph osd tree`


编辑：
```
ceph osd getcrushmap -o {compiled-crushmap-filename} # 获取
crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename} # 反编译
crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename} # 编译
ceph osd setcrushmap -i {compiled-crushmap-filename} # 注入
```

pool 中使用：
`ceph osd pool get <poolname> crush_rule`
`ceph osd pool set <poolname> crush_rule <rule>`

## Data Distribution
[![img](https://www.usenix.org/legacy/event/osdi06/tech/full_papers/weil/weil_html/fig/dist.png)](https://www.usenix.org/legacy/event/osdi06/tech/full_papers/weil/weil_html/index.html)

file -> object:
ino: file id
ono: object index in file （默认 4M 切分）
oid: object id (ino,ono)

object -> pg:
mask: pg 总数 - 1
pgid: hash(oid) & mask

pg -> osd:
osd ids: crush(pgid)

ex:
```
rados -p pool1 put object1 ./testfile
rados df
rados -p pool1 ls | grep object1
ceph osd map pool1 object1
# osdmap e310 pool 'pool1' (6) object 'object1' -> pg 6.f85a416a (6.6a) -> up ([1,4], p1) acting ([1,4], p1)
# e310: epoch number, 类似版本号
# pool 'pool1' (6): 所在 pool
# object 'object1': 对象名
# pg 6.f85a416a (6.6a): pg 信息
# up ([1,4], p1) acting ([1,4], p1): pg 在 osd [1,4] 上，1 是 primary osd

ceph pg dump | grep 6.6a
# PG_STAT OBJECTS MISSING_ON_PRIMARY DEGRADED MISPLACED UNFOUND BYTES    LOG DISK_LOG STATE        STATE_STAMP                VERSION REPORTED UP      UP_PRIMARY ACTING  ACTING_PRIMARY LAST_SCRUB SCRUB_STAMP                LAST_DEEP_SCRUB DEEP_SCRUB_STAMP           SNAPTRIMQ_LEN
# 6.6a          3                  0        0         0       0 16908288  49       49 active+clean 2019-07-26 16:39:08.570053  310'49  310:120   [1,4]          1   [1,4]              1        0'0 2019-07-26 11:17:34.543304             0'0 2019-07-26 11:17:34.543304             0
ceph pg 6.6a query
ceph osd find 1
```

根据 pool/osd 查询 pg：
`ceph pg ls-by-pool <pool-name>`
`ceph pg ls-by-osd osd.<id>`

## Usage
### RBD
创建 `rbd create <pool-name>/<image-name> --size 102400`
映射 `rbd map <pool-name>/<image-name>` 创建rbd设备 `/dev/rbd0`
查看 `rbd ls <pool-name>`, `rbd info <pool-name>/<image-name>`

快照（只读）
```
rbd snap create <pool_name>/<image_name>@<snapshot_name> # 创建
rbd snap rollback <pool_name>/<image_name>@<snapshot_name> # 回滚
```

克隆快照到新的 image（COW）
```
rbd snap protect <pool_name>/<image_name>@<snapshot_name> # 需先保护
rbd clone <pool_name>/<image_name>@<snapshot_name> <dest_pool_name>/<dest_image_name>
rbd snap unprotect <pool_name>/<image_name>@<snapshot_name>
```

克隆依赖原快照，查看快照的所有克隆
```
rbd children <pool_name>/<image_name>@<snapshot_name>
rbd flatten <dest_pool_name>/<dest_image_name> # 解除依赖，打平不再COW
```

### CephFS
```
mount -t ceph <mon_ip>:6789:/ /mnt/data -o name=admin,secret=AQCdTTBdLcUKAhAA0O0ckqyolPnB7r6hjHcBRg==
```

### Config
config 来源：

* default: 内置默认值
* mon: monitor 的中心配置DB
* file: node 上的文件 `/etc/ceph/ceph.conf`
* env: 环境变量
* cmdline: 命令行
* override: 运行时修改

显示某个实例（全部来源）`ceph config show <who>`, who: `mon.node1`, `osd.2`
显示某个实例（默认值）`ceph config show-with-defaults <who>`
显示来自中心配置DB `ceph config dump/get <who>`
设置到中心配置DB `ceph config set <who> <option> <value>`
运行时修改 `ceph tell mon.\* injectargs '--mon-allow-pool-delete=true'`

### Tuning
[Configuring Ceph from CERN IT Storage Group](https://indico.cern.ch/event/588794/contributions/2374222/attachments/1383112/2103509/Configuring_Ceph.pdf)
https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments
https://www.flashmemorysummit.com/English/Collaterals/Proceedings/2018/20180808_SOFT-202-1_Liu.pdf
https://yourcmc.ru/wiki/Ceph_performance
https://ceph.io/community/bluestore-default-vs-tuned-performance-comparison
https://blog.csdn.net/lzw06061139/article/details/51320138
https://www.jianshu.com/p/dd572541df2e

### OP
列出所有服务 `systemctl status ceph\*.service ceph\*.target`
启动所有osd实例 `systemctl start ceph-osd.target`
查看某个osd/mon实例状态 `systemctl status ceph-osd@1` `systemctl status ceph-mon@node1`
daemon 实例操作：`ceph daemon <who> help` / `ceph tell <who> help`
mon 操作：`ceph osd --help`
报告及进度：`ceph report`, `ceph progress`

#### 删除 osd
1. `ceph osd out {osd-num}`
1. osd host: `systemctl stop ceph-osd@{osd-num}`, `systemctl disable ceph-osd@{osd-num}`
1. `ceph osd purge {id}`
1. 若 `/etc/ceph/ceph.conf` 下有具体这个 osd 配置，需删除并同步到其他节点上，并重启 `systemctl restart ceph-osd.target`
1. 若要删除节点上所有组件 deploy host: `ceph-deploy purge hostname`, `ceph-deploy purgedata hostname`
1. 删除对应的 lvm, osd host: `lvremove /dev/ceph-...`, `vgremove ceph-...`

## Reference
http://docs.ceph.com
https://xiaqunfeng.gitbooks.io/ceph-practice-handbook

</xmp>
<script src="../js/strapdown.js"></script>
</html>
