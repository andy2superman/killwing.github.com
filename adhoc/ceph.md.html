<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>Ceph</title>
<xmp theme="united" style="display:none;">
## Install
安装 ceph-deploy
```
ceph_stable_release=nautilus # v14
wget -q -O- 'http://mirrors.163.com/ceph/keys/release.asc' | apt-key add -
echo deb https://mirrors.163.com/ceph/debian-${ceph_stable_release}/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list
apt-get update && apt-get install ceph-deploy
```

保证 node 之间 ssh 互通，创建 cluster
```
ceph-deploy new node1 node2 node3
ceph-deploy install node1 node2 node3
```
install 之前，在生成的 ceph.conf [global] 中加入 `osd pool default size = 2` 修改默认副本数。

```
ceph-deploy mon create-initial # 创建初始 mon
ceph-deploy admin node1 node2 node3 # 拷贝鉴权配置文件
```

增加 OSD
```
ceph-deploy disk zap node1 /dev/sdb # 准备磁盘，删除分区等
ceph-deploy osd create node1 --data /dev/sdb # 默认 bluestore, Block Data & WAL & DB 在同一块盘
```

* osd 基本元信息在 `/var/lib/ceph/osd/ceph-{idx}`
* 如果 Block 和 DB 分开放，DB 大小不应小于 Block 的 4%

增加 manager
```
ceph-deploy mgr create node1:mgr1
```

创建 pool
```
ceph osd pool create <poolname> <int[0-]> {<int[0-]>} {replicated|erasure} {<erasure_code_profile>} {<rule>} {<int>}
ceph osd pool set <poolname> size 3 # 改为3副本
ceph osd pool application enable <poolname> rbd # for rbd
```
[pg 计算](https://ceph.com/pgcalc/)

## LVM
* PE: 物理区，PV 中分配的最小单元（默认4M）
* PV: 物理卷，对应物理硬盘或分区
  * pvcreate, pvs, pvdisplay, pvremove
* VG: 卷组，包含一个或多个 PV，所有 PV 的 PE 大小相同
  * vgcreate, vgs, vgdisplay, vgextend, vgreduce
* LV: 逻辑卷，在 VG 里面分割，大小可动态改变（为 PE 整数倍）
  * lvcreate, lvs, lvdisplay, lvextend, lvreduce
  * device 路径在 /dev/{vg}/{lv}，后可以格式化挂载使用

## Crush Map
```
# begin crush map
tunable choose_local_tries 0
tunable choose_local_fallback_tries 0
tunable choose_total_tries 50
tunable chooseleaf_descend_once 1
tunable chooseleaf_vary_r 1
tunable chooseleaf_stable 1
tunable straw_calc_version 1
tunable allowed_bucket_algs 54

# devices 对应每个 osd
device 0 osd.0 class ssd
device 1 osd.1 class ssd
device 2 osd.2 class hdd
device 3 osd.3 class hdd

# types 桶类型, 表示 topo 位置
type 0 osd
type 1 host
type 2 chassis
type 3 rack
type 4 row
type 5 pdu
type 6 pod
type 7 room
type 8 datacenter
type 9 region
type 10 root

# buckets 桶
host node1 {
        id -3
        id -4 class ssd
        id -7 class hdd
        alg straw2
        hash 0  # rjenkins1
        item osd.0 weight 1 # 权重，一般 1T 为 1
        item osd.2 weight 1
}
host node2 {
        id -5
        id -6 class ssd
        id -8 class hdd
        alg straw2
        hash 0  # rjenkins1
        item osd.1 weight 1
        item osd.3 weight 1
}

root default {
        id -1
        id -2 class ssd
        id -9 class hdd
        alg straw2
        hash 0  # rjenkins1
        item node1 weight 2
        item node2 weight 2
}

# rules
rule replicated_rule {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default # 起始桶
        step chooseleaf firstn 0 type host # 0 表示选择 pool-num-replicas 个桶
        step emit # 输出当前值，清空栈
}

rule ssd {
        id 0
        type replicated
        min_size 1
        max_size 10
        step take default class ssd # 只选择匹配 ssd 的
        step chooseleaf firstn 0 type host
        step emit
}

# end crush map
```

rule placement:
```
tack(a)
choose
    choose firstn {num} type {bucket-type} # 选出 num 个指定类型的桶
    chooseleaf firstn {num} type {bucket-type} # 先选出 num 个指定类型的桶，然后在桶中选择一个叶子结点
        if {num} == 0, choose pool-num-replicas buckets (all available).
        if {num} > 0 && < pool-num-replicas, choose that many buckets.
        if {num} < 0, it means pool-num-replicas - {num}.
emit
```
查看树：`ceph osd tree`


编辑：
```
ceph osd getcrushmap -o {compiled-crushmap-filename} # 获取
crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename} # 反编译
crushtool -c {decompiled-crush-map-filename} -o {compiled-crush-map-filename} # 编译
ceph osd setcrushmap -i {compiled-crushmap-filename} # 注入
```

pool 中使用：
`ceph osd pool get <poolname> crush_rule`
`ceph osd pool set <poolname> crush_rule <rule>`

## Usage
### RBD
创建 `rbd create <pool-name>/<image-name> --size 102400`
映射 `rbd map <pool-name>/<image-name>` 创建rbd设备 `/dev/rbd0`

快照（只读）
```
rbd snap create <pool_name>/<image_name>@<snapshot_name> # 创建
rbd snap rollback <pool_name>/<image_name>@<snapshot_name> # 回滚
```

克隆快照到新的 image（COW）
```
rbd snap protect <pool_name>/<image_name>@<snapshot_name> # 需先保护
rbd clone <pool_name>/<image_name>@<snapshot_name> <dest_pool_name>/<dest_image_name>
rbd snap unprotect <pool_name>/<image_name>@<snapshot_name>
```

克隆依赖原快照，查看快照的所有克隆
```
rbd children <pool_name>/<image_name>@<snapshot_name>
rbd flatten <dest_pool_name>/<dest_image_name> # 解除依赖，打平不再COW
```

## Reference
http://docs.ceph.com
https://xiaqunfeng.gitbooks.io/ceph-practice-handbook

</xmp>
<script src="../js/strapdown.js"></script>
</html>
