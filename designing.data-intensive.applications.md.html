<!DOCTYPE html>
<html>
<meta charset="utf-8" />
<title>Designing Data-Intensive Applications</title>
<xmp theme="readable" style="display:none;" toc="true">
# 3 Storage and Retrieval
## Data Structures That Power Your Database
log: append-only 顺序记录的数据文件。

 * 顺序写，性能高，尤其在机械磁盘上
 * 并发控制和 crash 恢复相对容易。
 * 合并以避免数据碎片化问题。

存储系统里的一个重要权衡：精心选择的索引可以加速读（查询），但每个索引都会使写变慢。

### Hash Indexes
设计：

* 在内存里用 hash 表作为数据索引，元素结构为`{key: location}`。在硬盘上用 log 存放数据，顺序即为写入先后的顺序。
* 操作：
 * 增：追加新增的记录。
 * 删：追加删除标记`tombstone`记录。
 * 改：追加更新的记录。
 * 查：按从新到旧的顺序，在每个段的索引里查找。
* 分段：硬盘上数据文件分段避免过大，每段内存里都对应一个 hash 表索引。
* 压缩：对于重复的 key 数据，丢弃老的，保留最新的。
* 合并：把多个段合成一个。
* 特殊处理：
 * crash 恢复：需要重建 hash 索引，用 snapshot 加速。
 * 坏记录：chechsum 校验。
 * 并发控制：每个段单写多读。

局限性：

* hash 索引必须能全部放到内存中，不适合巨量 key 数目的情况。
* 根据 key 范围查询效率低。

实现：Bitcask
[riak](https://github.com/basho/riak)

### SSTables and LSM-Trees
[SSTables (Sorted String Table)](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf)：log 存储时按 key 的顺序，且每个段中的 key 不重复（压缩过）。

* 段合并类似归并排序，更有效率。
* 内存中 hash 索引可以是稀疏的而不用存放所有 key，如果要查找的 key 不在索引里，可以先找到前一个较近的被索引 key 的位置再往后 scan。
* 利用相邻稀疏 key 之间的范围，可以把数据分块压缩存储，每个 key 就指向对应块的位置。

构建和维护 SSTable:

* 写入时，先加到内存中有序的平衡树(实现：红黑，B-Tree, [skiplist](https://www.epaperpress.com/sortsearch/download/skiplist.pdf))上(`memtable`)。
* 当 `memtable` 大小超过阈值时，格式化为 SSTable 文件（即一个段）写入硬盘。后续写时，使用新的 `memtable` 实例，旧的丢弃。
* 读时，先在 `memtable` 中查找 key，再按从新到旧的顺序在 SSTable 文件中查找。可以用 bloom filter 优化查找。
* 后台定期合并压缩 SSTable 文件 (size-tiered/leveled)。
* 避免 crash 丢失内存数据，用单独的一个 log 按写入顺序保存每次写入 (WAL)。

实现：[LSM-Tree (Log Structured Merge Tree)](http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf)
[levelDB](https://github.com/golang/leveldb), RocksDB, Cassandra, [badger](https://github.com/dgraph-io/badger)

### B-Trees
B-Tree 将数据分成固定大小的页（4KB）作为树的节点，读写都以页为单位。

每个页都有 ref 到其他页，key 都有序且表示连续的范围。
`| ref1 | 100 | ref2 | 200 | ref3 | 300 | ref4 | 400 | ref5 | 500 | ref6 |`
ref1: key < 100
ref2: 100 <= key < 200

每个页中 ref 的数目固定，称为 `branching factor`。
叶子节点包含实际的 key 和 value （或 value 的位置）。
平衡：有 n 个 key 的 B-Tree，其深度为 `O(log n)`。
使用 WAL 防止写多个页时 crash。

增：如果页空间足够，直接添加；否则把所在的页分割为两个再添加并更新父节点的引用范围（写回三个页到硬盘）。
删：
改：更新叶子节点中 key 对应的值，写回整个页到硬盘（对于机械磁盘，原地覆写）。
查：从根节点开始，一直找到具体的叶子节点中的 key。

其他优化：

* 使用写时复制代替 WAL 作为 crash 恢复。
* 表示 key 范围的页，可以用缩写以节省空间。
* 相邻的 key 范围的页，在硬盘上可以不相邻。
* 页节点上可以存额外的指针，指向左右兄弟节点。
* 变种如 fractal trees (B+ tree)，借鉴了一些 log 结构的做法。

实现：
[boltDB](https://github.com/boltdb/bolt)

### Comparing B-Trees and LSM-Trees
LSM-trees 相比 B-Trees 来说写更快，但读较慢。

LSM-trees 优点：

* 写放大：一次写入数据库导致多次写入硬盘。影响写性能的主要因素。
* 较高的写入吞吐量，因为写放大不高和顺序写。
* 文件小，存储开销少效率高，碎片化少。
* 较少的写放大问题和少碎片化也对 SSD 友好。

LSM-trees 缺点：

* 后台的压缩过程可能影响正在读写的性能。
* 压缩导致较高的写入吞吐量，抢占有限的写入带宽。
* 如果配置不小心，可能导致压缩的速度跟不上初始写入的速度。
* key 是分散在不同的段中的，相比 B-trees 不容易提供强事务语义。

### Other Indexing Structures
(TODO)

# 8 The Trouble with Distributed Systems
## Faults and Partial Failures
在分布式系统中，可能有一些系统组件不可预料地出错，而其他组件工作正常，这称为局部错误，局部错误具有不确定性。

### Cloud Computing and Supercomputing
两种大规模计算机系统的设计哲学：

* 一种是高性能计算／超算（HPC）。
* 另一种是云计算。
* 传统的企业数据中心介于两者之间。

它们处理错误的方式不同：

* 超算：更像单机，局部出错就认为整个系统出错，进而可以全部重启从上一个检查点重新开始。
* 云计算：要求时刻在线，可用性高，容错的，局部错误系统仍然可用。

分布式系统中错误是常见的，因此错误处理是软件设计的重要部分，要在不可靠的组件之上构建可靠的系统。

## Unreliable Networks
因特网和数据中心的局域网是异步的分包网络，如果一个node发送消息给另一个node，这种网络并不能保证到达的时间，甚至能否到达。
通常的处理方式是：响应和超时机制。

### Network Faults in Practice
### Detecting Faults
某些特定情况下得到错误反馈：

* node 在运行但进程不在监听目的端口，TCP 连接返回 RST / FIN。
* 进程被杀或 crash，监控脚本通知其他 node。
* 通过网络交换机硬件的管理接口侦测链路错误。
* 路由器发现 IP 地址不可达，返回 ICMP 目的地不可达。

### Timeouts and Unbounded Delays
网络中包的延迟主要是由于队列机制（异步）：

* 网络交换机排队相同目的地的包。
* 包到达 node，如果CPU 正忙，OS 将排队请求直到应用可以处理。
* 当 CPU 被其他虚拟机占用，虚拟机监控排队网络请求。
* TCP 流控制，当数据被发送到网络前进行排队。

### Synchronous Versus Asynchronous Networks
传统电话网络是同步的，预分配信道，没有中间队列，最大端到端的延迟是确定的(bounded delay)。
电话网 circuit-switched vs 以太网 packet-switched。
必须假设网络是可能发生阻塞，排队，和无限延迟的，所以没有对超时长短的正确时间，需要由测试决定。

## Unreliable Clocks
在一个分布式系统中，涉及到多个机器，很难确定事件的发生顺序。
最常用的时间同步机制是 NTP。

### Monotonic Versus Time-of-Day Clocks
两种时钟：

* time-of-day
 * 又称墙上时钟，主要用来根据日历返回当前的日期和时间。`clock_gettime(CLOCK_REALTIME)`
 * 通常由 NTP 来同步。
 * 可能发生时间回拨，不宜用来测量持续时间。
 * 精度低。
* monotonic
 * 单调时钟，主要用来测量时间间隔。`clock_gettime(CLOCK_MONOTONIC)`
 * 不会发生回拨。
 * NTP 用来调整时钟前进的频率。
 * 时钟的绝对值，以及比较不同机器的时钟没有意义，不需要“同步”。
 * 精度高。

### Clock Synchronization and Accuracy
即使有 NTP，获取正确的当前时间并不是准确和可靠的。

### Relying on Synchronized Clocks
任何时钟偏移其他节点太多的节点都应被宣布死亡并从集群中移除。

last write wins (LWW) 的问题：

* 可能丢失写（被时钟快的节点的写覆盖）
* 不能区分很快的连续写和真正的同时写
* 在某个精度下的同时写是可能的

逻辑时钟并不测量具体的时间或间隔（墙上时钟和单调时钟都是物理时钟），而是关心事件发生的相对顺序，可以用来排序事件。
生成全局单调递增的事务 ID 是困难的，使用同步的墙上时间的时间戳来实现会有时钟不准确的问题。
使用时钟同步来完成分布式事务语义还属研究领域，未在主流数据库中实现。

## Process Pauses
分布式系统里面的节点必须假设其执行可能在任意点甚至函数执行中间暂停一定时间，然后再继续执行（而未再检查时钟）。

## Knowledge, Truth, and Lies
分布式系统：没有共享内存，只有通过不可靠并有延迟网络的消息传递，系统可能经受部分错误，不可靠的时钟，及进程暂停。
在一定的系统模型（行为假设）下，算法可以被证明是功能正确的。这表明可靠的行为是可以达成的，即使底层的系统模型只提供很少的保证。

### The Truth Is Defined by the Majority
分布式系统不能依靠单个节点（来做决定），而是节点之间投票决定(最少票数：quorum)。
通常 quorum 数目为大于一半节点的多数。所以总节点数一般为单数，三个节点可以容忍一个节点失败，五个可以容忍两个。

分布式锁（只有一个/独占操作）：
fencing: 每次获取 lock/lease 时同时获取一个每次递增的 fencing token，服务端只允许带有相同或递增的 token 的请求。
（可以保证操作顺序按照获取锁的顺序，不能保证并发冲突）

### Byzantine Faults
一个系统是 Byzantine 容错的，指在即使一些节点功能失常不遵守协议，或者有恶意攻击者干扰网络的情况下也能正确工作。
Byzantine 容错的协议非常复杂，部署成本很高，不切实际。但一般可以认为数据中心不会出现此类错误。

### System Model and Reality
system model:

* 计时假设
 * 同步：确定有上限的网络延迟，进程暂停，时钟错误。
 * 半同步：大部分时候是确定的，有时候会超出上限。常见模型，比较接近现实中的系统。
 * 异步：算法不能做任何确定性的假设。
* 节点错误假设
 * 只有 crash，并不会恢复。
 * crash 后，一定时间内可以恢复，有持久存储可以存状态，但内存中的状态会丢失。常见模型。
 * Byzantine 错误，可能做出任意行为。

定义算法的正确性：通过描述其属性。比如 fencing token 的唯一性，递增性，和可用性。

* 安全性: 如果违反，在某个时间点此属性被破坏（不符合），且不能恢复。（一致性）
* 活动性: 在某个时间点可能不符合，但可能在将来满足。（可用性）

# 9 Consistency and Consensus
## Consistency Guarantees
最终一致性：写完以后，等待一个不确定的时间，最终所有的读请求都能返回其值。是一个较弱的保证。
强一致性：写完以后所有读请求都立即能读到其值。性能不如更弱的保证。

## Linearizability
线性一致性是一种强一致性保证。

### What Makes a System Linearizable?
系统看上去只有一份数据，没有其他备份或缓存可以读。
操作具有原子性。比如写入使用 CAS 操作。

串行化：事务隔离属性，保证事务执行像按一定顺序执行。

### Relying on Linearizability

</xmp>
<script src="js/strapdown.js"></script>
</html>
